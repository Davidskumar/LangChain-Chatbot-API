# -*- coding: utf-8 -*-
"""LangChain-Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z9EyV4KQq37De3YZFhRYpLFDOY60fcDQ
"""

!pip install langchain langchain-community chromadb flask
!pip install langchain-huggingface flask
!pip install flask-restful

!pip install pyngrok
!ngrok authtoken #[Your access token]

from huggingface_hub import login
login()

from flask import Flask, request, jsonify
import threading

app = Flask(__name__)

@app.route("/chat", methods=["POST"])
def chat():
    data = request.get_json()
    if not data or "message" not in data:
        return jsonify({"error": "Missing 'message' key in request"}), 400
    return jsonify({"response": f"Received: {data['message']}"})

def run_flask():
    print("ðŸš€ Starting Flask server...")
    app.run(host="0.0.0.0", port=5000, debug=True, use_reloader=False)

thread = threading.Thread(target=run_flask)
thread.start()

!lsof -i :5000

!kill -9 40150

!huggingface-cli whoami

import os
os.environ["USER_AGENT"] = "ChatFlask/1.0"

from google.colab import userdata
HF_TOKEN = ""  # Replace with your actual token
userdata.get = lambda key: HF_TOKEN if key == "HF_TOKEN" else None

import os
import time
import requests
import threading
from google.colab import userdata
from flask import Flask, request, jsonify
from flask_cors import CORS
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEndpoint
from langchain.chains import ConversationalRetrievalChain
from huggingface_hub import login

#  Hugging Face API Authentication
# HUGGINGFACEHUB_API_TOKEN = ""
# os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN
# login(HUGGINGFACEHUB_API_TOKEN)

HUGGINGFACEHUB_API_TOKEN = userdata.get('HF_TOKEN')  # Retrieve stored token
if not HUGGINGFACEHUB_API_TOKEN:
    raise ValueError(" Hugging Face API Token not found! Please set it in Colab.")

os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN
login(HUGGINGFACEHUB_API_TOKEN)

app = Flask(__name__)
CORS(app)

def load_website_data():
    url = ["https://brainlox.com/courses/category/technical"]
    loader = WebBaseLoader(url)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    return text_splitter.split_documents(documents)

def create_embeddings(docs):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return Chroma.from_documents(docs, embeddings)

def load_llm():
    return HuggingFaceEndpoint(
        repo_id="mistralai/Mistral-7B-Instruct",
        huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
        task="text-generation",
        temperature=0.7,
        max_length=512
    )

docs = load_website_data()
vector_store = create_embeddings(docs)
retriever = vector_store.as_retriever()
llm = load_llm()
qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever)

@app.route("/chat", methods=["POST"])
def chat():
    data = request.json
    user_query = data.get("message", "")

    if not user_query:
        return jsonify({"error": "No input provided"}), 400

    response = qa_chain.invoke({"question": user_query, "chat_history": []})

    model_answer = response.get("answer", "").strip()

    if not model_answer:
        return jsonify({"error": "No response generated"}), 500

    return jsonify({"response": model_answer})
##############
@app.route("/test_model", methods=["POST"])
def test_model():
    data = request.json
    user_query = data.get("message", "")

    if not user_query:
        return jsonify({"error": "No input provided"}), 400

    model_response = llm.invoke(user_query)

    print("DEBUG: Model Output ->", model_response)

    return jsonify({"response": model_response})
#############
@app.route("/test_retriever", methods=["POST"])
def test_retriever():
    data = request.json
    user_query = data.get("message", "")

    if not user_query:
        return jsonify({"error": "No input provided"}), 400

    retrieved_docs = retriever.get_relevant_documents(user_query)

    print("DEBUG: Retrieved Documents ->", retrieved_docs)

    return jsonify({"retrieved_docs": [doc.page_content for doc in retrieved_docs]})
################
@app.route("/test_qa_chain", methods=["POST"])
def test_qa_chain():
    data = request.json
    user_query = data.get("message", "")

    if not user_query:
        return jsonify({"error": "No input provided"}), 400

    response = qa_chain.invoke({"question": user_query, "chat_history": []})

    print("DEBUG: QA Chain Output ->", response)

    return jsonify({"response": response.get("answer", "")})
############
@app.route("/health", methods=["GET"])
def health_check():
    try:
        test_query = "What are some technical courses?"
        response = qa_chain.invoke({"question": test_query, "chat_history": []})

        return jsonify({"status": "ready", "sample_response": response.get("answer", "")})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

def reload_embeddings():
    global vector_store, retriever, qa_chain
    print(" Reloading embeddings...")

    docs = load_website_data()
    vector_store = create_embeddings(docs)
    retriever = vector_store.as_retriever()
    llm = load_llm()

    qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever)
    print(" Reload Complete!")

@app.before_request
def initialize():
    reload_embeddings()
    time.sleep(2)

def run_flask():
    print(f"ðŸš€ Flask Server Running")
    app.run(host="0.0.0.0", port=5000, debug=False, use_reloader=False)

thread = threading.Thread(target=run_flask)
thread.start()

from pyngrok import ngrok
ngrok.kill()
public_url = ngrok.connect(5000).public_url
print(f"ðŸš€ Flask is live at: {public_url}")

import requests

ngrok_url = ""  # Replace with actual ngrok URL
data = {"message": "List some technical courses."}

response = requests.post(ngrok_url, json=data)
print(response.json())

!pkill -f flask